{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the Neural Network Model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# 2. Implement Elastic Weight Consolidation (EWC)\n",
        "class EWC:\n",
        "    def __init__(self, model, dataset, lambda_ewc):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.lambda_ewc = lambda_ewc\n",
        "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        self._means = {}\n",
        "        self._fisher = {}\n",
        "        self._compute_fisher_and_means()\n",
        "\n",
        "    def _compute_fisher_and_means(self):\n",
        "        self.model.eval()\n",
        "        for n, p in self.params.items():\n",
        "            self._means[n] = p.clone().detach()\n",
        "            self._fisher[n] = torch.zeros_like(p)\n",
        "\n",
        "        data_loader = DataLoader(self.dataset, batch_size=1, shuffle=True)\n",
        "        for x, y in data_loader:\n",
        "            self.model.zero_grad()\n",
        "            output = self.model(x.view(x.size(0), -1))\n",
        "            loss = nn.CrossEntropyLoss()(output, y)\n",
        "            loss.backward()\n",
        "            for n, p in self.params.items():\n",
        "                self._fisher[n] += p.grad.data ** 2 / len(data_loader)\n",
        "\n",
        "        for n, p in self._fisher.items():\n",
        "            self._fisher[n] = p / len(self.dataset)\n",
        "\n",
        "    def penalty(self):\n",
        "        loss = 0\n",
        "        for n, p in self.params.items():\n",
        "            _loss = self._fisher[n] * (p - self._means[n]) ** 2\n",
        "            loss += _loss.sum()\n",
        "        return self.lambda_ewc * loss\n",
        "\n",
        "# 3. Training Loop with EWC and Learning Rate Scheduler\n",
        "def train_ewc(model, datasets, epochs, learning_rate, lambda_ewc):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Example scheduler\n",
        "\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "        ewc = EWC(model, dataset, lambda_ewc)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for inputs, labels in data_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs.view(inputs.size(0), -1))\n",
        "                loss = criterion(outputs, labels)\n",
        "                if i > 0:\n",
        "                    loss += ewc.penalty()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            # Step the learning rate scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Task {i+1}, Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}, LR: {scheduler.get_last_lr()}\")\n",
        "\n",
        "# 4. Evaluation Metrics\n",
        "def evaluate_task(model, dataset):\n",
        "    data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs.view(inputs.size(0), -1))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def evaluate_all_tasks(model, datasets):\n",
        "    accuracies = []\n",
        "    for i, dataset in enumerate(datasets):\n",
        "        accuracy = evaluate_task(model, dataset)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Accuracy on Task {i+1}: {accuracy:.2f}%\")\n",
        "    average_accuracy = sum(accuracies) / len(accuracies)\n",
        "    print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
        "    return accuracies, average_accuracy\n",
        "\n",
        "def calculate_forgetting(initial_accuracies, final_accuracies):\n",
        "    forgetting = []\n",
        "    for i in range(len(initial_accuracies)):\n",
        "        forgetting.append(initial_accuracies[i] - final_accuracies[i])\n",
        "    avg_forgetting = sum(forgetting) / len(forgetting)\n",
        "    print(f\"Average Forgetting: {avg_forgetting:.2f}%\")\n",
        "    return forgetting, avg_forgetting\n",
        "\n",
        "def calculate_forward_transfer(model, datasets):\n",
        "    forward_transfer = []\n",
        "    for i in range(1, len(datasets)):\n",
        "        fresh_model = SimpleNN(input_size=784, hidden_size=256, output_size=2)\n",
        "        train_ewc(fresh_model, [datasets[i]], epochs=10, learning_rate=0.01, lambda_ewc=0)\n",
        "        fresh_accuracy = evaluate_task(fresh_model, datasets[i])\n",
        "\n",
        "        current_accuracy = evaluate_task(model, datasets[i])\n",
        "        transfer = current_accuracy - fresh_accuracy\n",
        "        forward_transfer.append(transfer)\n",
        "        print(f\"Forward Transfer for Task {i+1}: {transfer:.2f}%\")\n",
        "    avg_forward_transfer = sum(forward_transfer) / len(forward_transfer)\n",
        "    print(f\"Average Forward Transfer: {avg_forward_transfer:.2f}%\")\n",
        "    return forward_transfer, avg_forward_transfer\n",
        "\n",
        "# 5. Example Usage with Fashion-MNIST Dataset\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "fashion_mnist = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split Fashion-MNIST into two tasks:\n",
        "# Task 1: Classifying T-shirts/Tops (label 0) vs Trousers (label 1)\n",
        "# Task 2: Classifying Pullovers (label 2) vs Dresses (label 3)\n",
        "\n",
        "indices_task1 = np.where((fashion_mnist.targets == 0) | (fashion_mnist.targets == 1))[0]\n",
        "indices_task2 = np.where((fashion_mnist.targets == 2) | (fashion_mnist.targets == 3))[0]\n",
        "\n",
        "dataset_task1 = Subset(fashion_mnist, indices_task1)\n",
        "dataset_task2 = Subset(fashion_mnist, indices_task2)\n",
        "\n",
        "# Re-label the tasks to have labels 0 and 1\n",
        "for i in range(len(dataset_task1)):\n",
        "    dataset_task1.dataset.targets[dataset_task1.indices[i]] = int(dataset_task1.dataset.targets[dataset_task1.indices[i]] == 1)\n",
        "\n",
        "for i in range(len(dataset_task2)):\n",
        "    dataset_task2.dataset.targets[dataset_task2.indices[i]] = int(dataset_task2.dataset.targets[dataset_task2.indices[i]] == 3)\n",
        "\n",
        "datasets = [dataset_task1, dataset_task2]\n",
        "\n",
        "# Initialize the model and train\n",
        "model = SimpleNN(input_size=784, hidden_size=256, output_size=2)\n",
        "train_ewc(model, datasets, epochs=10, learning_rate=0.01, lambda_ewc=0.4)\n",
        "\n",
        "# Evaluate performance after learning all tasks\n",
        "initial_accuracies = evaluate_all_tasks(model, datasets)[0]\n",
        "final_accuracies = evaluate_all_tasks(model, datasets)[0]\n",
        "forgetting, avg_forgetting = calculate_forgetting(initial_accuracies, final_accuracies)\n",
        "forward_transfer, avg_forward_transfer = calculate_forward_transfer(model, datasets)\n",
        "\n",
        "# Summary of results\n",
        "print(f\"Average Accuracy: {sum(final_accuracies)/len(final_accuracies):.2f}%\")\n",
        "print(f\"Average Forgetting: {avg_forgetting:.2f}%\")\n",
        "print(f\"Average Forward Transfer: {avg_forward_transfer:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTF9vEDh1oiG",
        "outputId": "dfb28099-5a8b-4b2b-882c-ba1c2ee2f900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1, Epoch 1, Loss: 0.16201461815929158, LR: [0.01]\n",
            "Task 1, Epoch 2, Loss: 0.0644669945331964, LR: [0.01]\n",
            "Task 1, Epoch 3, Loss: 0.0517908925032045, LR: [0.01]\n",
            "Task 1, Epoch 4, Loss: 0.04648446631023383, LR: [0.01]\n",
            "Task 1, Epoch 5, Loss: 0.042838892379300074, LR: [0.001]\n",
            "Task 1, Epoch 6, Loss: 0.041561374313227754, LR: [0.001]\n",
            "Task 1, Epoch 7, Loss: 0.04076181133921714, LR: [0.001]\n",
            "Task 1, Epoch 8, Loss: 0.04047835495402204, LR: [0.001]\n",
            "Task 1, Epoch 9, Loss: 0.04026413850109786, LR: [0.001]\n",
            "Task 1, Epoch 10, Loss: 0.04003787631827823, LR: [0.0001]\n",
            "Task 2, Epoch 1, Loss: 0.7996512993853143, LR: [0.0001]\n",
            "Task 2, Epoch 2, Loss: 0.5752830529149543, LR: [0.0001]\n",
            "Task 2, Epoch 3, Loss: 0.4648738855377157, LR: [0.0001]\n",
            "Task 2, Epoch 4, Loss: 0.3955476814286506, LR: [0.0001]\n",
            "Task 2, Epoch 5, Loss: 0.34809246453198983, LR: [1e-05]\n",
            "Task 2, Epoch 6, Loss: 0.3267561170331975, LR: [1e-05]\n",
            "Task 2, Epoch 7, Loss: 0.32395265655631716, LR: [1e-05]\n",
            "Task 2, Epoch 8, Loss: 0.3212822937267892, LR: [1e-05]\n",
            "Task 2, Epoch 9, Loss: 0.3183227522259063, LR: [1e-05]\n",
            "Task 2, Epoch 10, Loss: 0.3143772186434015, LR: [1.0000000000000002e-06]\n",
            "Accuracy on Task 1: 94.01%\n",
            "Accuracy on Task 2: 87.08%\n",
            "Average Accuracy: 90.55%\n",
            "Accuracy on Task 1: 94.01%\n",
            "Accuracy on Task 2: 87.08%\n",
            "Average Accuracy: 90.55%\n",
            "Average Forgetting: 0.00%\n",
            "Task 1, Epoch 1, Loss: 0.17214709381632348, LR: [0.01]\n",
            "Task 1, Epoch 2, Loss: 0.09257343454048672, LR: [0.01]\n",
            "Task 1, Epoch 3, Loss: 0.08623684926870022, LR: [0.01]\n",
            "Task 1, Epoch 4, Loss: 0.08242078790282632, LR: [0.01]\n",
            "Task 1, Epoch 5, Loss: 0.07966317137346622, LR: [0.001]\n",
            "Task 1, Epoch 6, Loss: 0.07757946810724729, LR: [0.001]\n",
            "Task 1, Epoch 7, Loss: 0.07795266849186985, LR: [0.001]\n",
            "Task 1, Epoch 8, Loss: 0.07725163171206542, LR: [0.001]\n",
            "Task 1, Epoch 9, Loss: 0.07685831438333905, LR: [0.001]\n",
            "Task 1, Epoch 10, Loss: 0.07655652209700263, LR: [0.0001]\n",
            "Forward Transfer for Task 2: -10.38%\n",
            "Average Forward Transfer: -10.38%\n",
            "Average Accuracy: 90.55%\n",
            "Average Forgetting: 0.00%\n",
            "Average Forward Transfer: -10.38%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}